{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6665b6a5",
   "metadata": {},
   "source": [
    "# Estadistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa0ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from edge_analysis import process_rois \n",
    "process_rois(mag=\"20X\", parte=\"P\" , imagenes=[\"_original\"],zonas=[\"_Pelos_X\"],show=False,show_overview=True)\n",
    "process_rois(mag=\"50X\" , imagenes=[\"_original\"],show=False,show_overview=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98ad9775",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_50X_P2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 98\u001b[0m\n\u001b[1;32m     91\u001b[0m                 resultados[nombre][v] \u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resultados\n\u001b[1;32m     94\u001b[0m k_Data_50 \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m50 A2_1\u001b[39m\u001b[38;5;124m\"\u001b[39m: df_50X_A2,\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m50 A2_2\u001b[39m\u001b[38;5;124m\"\u001b[39m: df_50X_2_A2,\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m50 P_1\u001b[39m\u001b[38;5;124m\"\u001b[39m: df_50X_P,\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m50 P_2\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mdf_50X_P2\u001b[49m\n\u001b[1;32m     99\u001b[0m     }\n\u001b[1;32m    100\u001b[0m k_Data_20 \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m20 A2_1\u001b[39m\u001b[38;5;124m\"\u001b[39m:df_20X_A2,\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m20 P_1\u001b[39m\u001b[38;5;124m\"\u001b[39m:df_20X_P\n\u001b[1;32m    103\u001b[0m     }\n\u001b[1;32m    104\u001b[0m resultados_50 \u001b[38;5;241m=\u001b[39m procesar_datos(k_Data_50, mag\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_50X_P2' is not defined"
     ]
    }
   ],
   "source": [
    "from edge_analysis import process_rois \n",
    "df_50X_2_A2 = process_rois(mag=\"50X\", imagenes=[\"_binario\"],zonas=[\"_Pelos_X2\"],show=False,show_overview=False)\n",
    "df_50X_A2  =  process_rois(mag=\"50X\", imagenes=[\"_binario\"],show=False,show_overview=False)\n",
    "df_20X_A2  =  process_rois(mag=\"20X\", imagenes=[\"_binario\"],show=False,show_overview=False)\n",
    "df_50X_P   =  process_rois(mag=\"50X\", parte=\"P\" , imagenes=[\"_binario\"],zonas=[\"_Pelos_X\"],show=False,show_overview=False)\n",
    "df_20X_P   =  process_rois(mag=\"20X\", parte=\"P\" , imagenes=[\"_binario\"],zonas=[\"_Pelos_X\"],show=False,show_overview=False)\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import uncertainties as un\n",
    "from uncertainties import unumpy\n",
    "import pandas as pd\n",
    "import math\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "\n",
    "\n",
    "def pretty_unc(x, sig=1):\n",
    "    \"\"\"Devuelve una cadena del tipo '7300+/-300' para un ufloat o lista de ufloats.\"\"\"\n",
    "    def _format_single(val):\n",
    "        v, e = val.n, val.s\n",
    "        if e == 0:\n",
    "            return f\"{v}\"\n",
    "        exp = int(math.floor(math.log10(abs(e))))\n",
    "        err_rounded = round(e, -exp + (sig - 1))\n",
    "        val_rounded = round(v, -exp + (sig - 1))\n",
    "        if err_rounded.is_integer() and val_rounded.is_integer():\n",
    "            return f\"{int(val_rounded)}+/-{int(err_rounded)}\"\n",
    "        else:\n",
    "            return f\"{val_rounded}+/-{err_rounded}\"\n",
    "    if isinstance(x, (list, np.ndarray)):\n",
    "        return np.array([_format_single(xi) for xi in x])\n",
    "    else:\n",
    "        return _format_single(x)\n",
    "\n",
    "def extraer_datos(DF: pd.DataFrame, clave: str, valor: str, columna=\"distance_ufloat\", mag=50):\n",
    "    \"\"\"Filtra el DataFrame y calcula px/mm y distancia al centro según zona y magnificación.\"\"\"\n",
    "    Resultados = DF[DF[clave] == valor].copy()\n",
    "    if Resultados.empty:\n",
    "        return None  # Devuelve None si no hay datos coincidentes\n",
    "\n",
    "    Datos_np = Resultados[columna].to_numpy()\n",
    "    \n",
    "    if clave == \"zona\":\n",
    "        # Escalas de calibración según zona\n",
    "        if valor in [\"_Pelos_X\", \"_Pelos_X2\"]:\n",
    "            medida = 0.001567\n",
    "        elif valor == \"_Cuadrado_X\":\n",
    "            medida = 0.03093\n",
    "        elif valor == \"_Cuadrado_Y\":\n",
    "            medida = 0.03054\n",
    "        else:\n",
    "            return None  # valor no reconocido\n",
    "\n",
    "        k = [un.ufloat_fromstr(val) / medida for val in Datos_np]\n",
    "        Resultados[\"px/mm\"] = pd.Series(k, index=Resultados.index).values\n",
    "\n",
    "        # Tamaño de imagen según magnificación\n",
    "        L = 2980 if mag == 50 else 3040\n",
    "        L = 3040\n",
    "\n",
    "        # Calcular distancia al centro\n",
    "        Xpos2 = (Resultados[\"X_centro\"] - L / 2) ** 2\n",
    "        Ypos2 = (-Resultados[\"Y_centro\"] + L / 2) ** 2\n",
    "        r = np.sqrt(Xpos2 + Ypos2)\n",
    "        Resultados[\"distancia del centro\"] = r\n",
    "        Resultados['angulo'] = np.arctan2(Ypos2,Xpos2)\n",
    "\n",
    "        # Eliminar columnas innecesarias\n",
    "        cols_drop = [\"X_centro\", \"Y_centro\", \"efecto\", \"distance_mean\",\n",
    "                     \"distance_std\", \"zona\", \"roi\"]\n",
    "        Resultados.drop(columns=[c for c in cols_drop if c in Resultados.columns],\n",
    "                        inplace=True, errors=\"ignore\")\n",
    "    return Resultados\n",
    "\n",
    "def procesar_datos(data_dict, clave=\"zona\", valores=None, mag=50):\n",
    "    \"\"\"\n",
    "    Ejecuta extraer_datos automáticamente para todos los DataFrames y zonas.\n",
    "    Devuelve un diccionario con los resultados.\n",
    "    \"\"\"\n",
    "    if valores is None:\n",
    "        valores = [\"_Pelos_X\", \"_Pelos_X2\", \"_Cuadrado_X\", \"_Cuadrado_Y\"]\n",
    "\n",
    "    resultados = {}\n",
    "\n",
    "    for nombre, df in data_dict.items():\n",
    "        resultados[nombre] = {}\n",
    "        for v in valores:\n",
    "            res = extraer_datos(df, clave, v, mag=mag)\n",
    "            if res is not None and not res.empty:\n",
    "                resultados[nombre][v] = res\n",
    "\n",
    "    return resultados\n",
    "k_Data_50 = {\n",
    "    \"50 A2_1\": df_50X_A2,\n",
    "    \"50 A2_2\": df_50X_2_A2,\n",
    "    \"50 P_1\": df_50X_P,\n",
    "    \"50 P_2\": df_50X_P2\n",
    "    }\n",
    "k_Data_20 = {\n",
    "    \"20 A2_1\":df_20X_A2,\n",
    "    \"20 P_1\":df_20X_P\n",
    "    }\n",
    "resultados_50 = procesar_datos(k_Data_50, mag=50)\n",
    "resultados_20 = procesar_datos(k_Data_20, mag=20)\n",
    "resultados_50[\"50 P_1\"][\"_Pelos_X\"].drop(3,axis=0,inplace=True) #eliminar el outlier\n",
    "resultados_50[\"50 P_2\"][\"_Pelos_X\"].drop(1,axis=0,inplace=True) #eliminar el outlier\n",
    "resultados_50[\"50 A2_2\"][\"_Pelos_X2\"].drop(4,axis=0,inplace=True)\n",
    "\n",
    "def configurar_estilo_global():\n",
    "    \"\"\"Configura el estilo global para todos los gráficos\"\"\"\n",
    "    plt.rcParams.update({\n",
    "    'axes.grid': True,\n",
    "    'axes.grid.which': 'both',\n",
    "    'font.size': 14,  # Aumentado de 12 a 14\n",
    "    'axes.titlesize': 14,  # Aumentado\n",
    "    'axes.labelsize': 11,  # Aumentado\n",
    "    'legend.fontsize': 10,  # Aumentado\n",
    "    'xtick.labelsize': 10,  # Aumentado\n",
    "    'ytick.labelsize': 10,  # Aumentado\n",
    "    'grid.color': '#CCCCCC',\n",
    "    'grid.linestyle': '--',\n",
    "    'grid.alpha': 0.7,\n",
    "    'xtick.major.pad': 10,  # Espaciado para mejor legibilidad\n",
    "    'ytick.major.pad': 10,  # Espaciado para mejor legibilidad\n",
    "    'errorbar.capsize':3,\n",
    "    'xtick.major.bottom': True,\n",
    "    'xtick.minor.bottom': True,\n",
    "    'ytick.major.left': True,\n",
    "    'ytick.minor.left': True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea62145",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MAD_Detection(x,y,sigma,y_real):\n",
    "    MAD = stats.median_abs_deviation(y,scale=\"normal\")\n",
    "    outliers={\"i\":[],\"outliers\":[]}\n",
    "    i=-1\n",
    "    borrados =False\n",
    "    for y_i,x_i in zip(y,x):\n",
    "        i+=1\n",
    "        if abs(np.mean(y)-y_i)>3*MAD:\n",
    "            print(\"OUTLIER!!! : \",f\"({x_i},{y_i})\")\n",
    "            outliers[\"i\"].append(i)\n",
    "            outliers[\"outliers\"].append([x_i,y_i])\n",
    "            borrados = True\n",
    "    if borrados:\n",
    "        y_limpio = np.delete(y_real,outliers[\"i\"])\n",
    "        std_limpio = np.delete(sigma,outliers[\"i\"])\n",
    "        x_limpio = np.delete(x,outliers[\"i\"])\n",
    "        return unumpy.uarray(y_limpio,std_limpio),x_limpio,True,outliers[\"outliers\"]\n",
    "    else:\n",
    "        return unumpy.uarray(y_real,sigma),x,False,outliers[\"outliers\"]\n",
    "def U(x):\n",
    "    \"\"\"Función base para el cálculo de varianza\"\"\"\n",
    "    return np.array([1, x, x**2])\n",
    "def prediction_uncertainty(r, cov_matrix):\n",
    "    \"\"\"Calcula la incertidumbre de predicción para valores r\"\"\"\n",
    "    if isinstance(r, float):\n",
    "        return np.sqrt(U(r).T @ cov_matrix @ U(r))\n",
    "    else:\n",
    "        return np.array([np.sqrt(U(a).T @ cov_matrix @ U(a)) for a in r])\n",
    "def quadratic_model(x, a,b,c):\n",
    "    \"\"\"Modelo cuadrático: y = a + b*x + c*x²\"\"\"\n",
    "    return a + b*x + c*x**2\n",
    "\n",
    "def fit_quadratic(x, y, sigma_y):\n",
    "    \n",
    "    \"\"\"Realiza ajuste cuadrático ponderado y retorna resultados\"\"\"\n",
    "    coeffs,covariance_1 = curve_fit(quadratic_model,x,y,sigma=sigma_y,bounds=((0,0,0),(np.inf,np.inf,1))) \n",
    "    if coeffs[2]<1e-10:\n",
    "        covariance = np.zeros((3,3))\n",
    "        coeffs,covariance_1 = curve_fit(lambda x,a,b:quadratic_model(x,a,b,0),x,y,sigma=sigma_y,bounds=((0,0),(np.inf,np.inf))) \n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                covariance[i,j]=covariance_1[i,j]\n",
    "        coeffs=np.concatenate((coeffs,[0]))\n",
    "        d=2\n",
    "        if coeffs[1]<1e-10:\n",
    "            covariance = np.zeros((3,3))\n",
    "            coeffs,covariance_1 = curve_fit(lambda x,a:quadratic_model(x,a,0,0),x,y,sigma=sigma_y,bounds=(0,np.inf)) \n",
    "            for i in range(2):\n",
    "                for j in range(2):\n",
    "                    covariance[i,j]=0\n",
    "            covariance[0,0]=covariance_1[0,0]\n",
    "            coeffs=np.concatenate((coeffs,[0,0]))\n",
    "            d=1\n",
    "\n",
    "        return coeffs,covariance,d\n",
    "    else:\n",
    "        return coeffs, covariance_1,3\n",
    "def calculate_statistics(y_data, y_pred,ddof):\n",
    "    \"\"\"Calcula estadísticas del ajuste\"\"\"\n",
    "    residuals = (y_data - y_pred)\n",
    "    ss_res = np.sum(residuals**2)\n",
    "    print(\"El error estandar de la regresión es: \",pretty_unc((ss_res/len(y_data))**0.5))\n",
    "    ss_tot = np.sum((y_data - np.mean(y_data))**2)\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "    chi2_red = np.dot(unumpy.nominal_values(residuals**2),unumpy.std_devs(y_data)**-2)/(len(y_data)-ddof)\n",
    "    chi2_red\n",
    "    return residuals, r_squared,chi2_red\n",
    "def print_fit_results(coeffs, cov_matrix, degrees_freedom):\n",
    "    \"\"\"Imprime resultados del ajuste de forma formateada\"\"\"\n",
    "    t_critical = stats.t.ppf(0.975, degrees_freedom)\n",
    "    errors = np.sqrt(np.diag(cov_matrix)) * t_critical\n",
    "    \n",
    "    coeffs_un = unumpy.uarray(coeffs,errors)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"RESULTADOS DEL AJUSTE CUADRÁTICO\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Coeficientes con incertidumbre (95% confianza):\")\n",
    "    print(f\"a (término constante): {coeffs_un[0]:.1uf}\")\n",
    "    print(f\"b (término lineal):    {coeffs_un[1]:.1uf}\") \n",
    "    print(f\"c (término cuadrático): {coeffs_un[2]:.2uf}\")\n",
    "    print(f\"\\nEcuación del ajuste:\")\n",
    "    print(f\"y = ({coeffs_un[0]:.1u}) + ({coeffs_un[1]:.1u})·r + ({coeffs_un[2]:.1u})·r²\")\n",
    "    print(f\"\\nValor crítico t(0.025, {degrees_freedom}) = {t_critical:.4f}\")\n",
    "    \n",
    "    return coeffs_un\n",
    "def apply_correction(y_data, x, coeffs,covariance):\n",
    "    \"\"\"\n",
    "    Aplica corrección: y_corregido = y - (f(x) - f(0))\n",
    "    Esto elimina la dependencia radial, centrando en f(0)\n",
    "    \"\"\"\n",
    "    t_student = stats.t.ppf(0.975,len(y_data)-1)\n",
    "    f_x = unumpy.uarray(quadratic_model(x, *coeffs),t_student*prediction_uncertainty(x,covariance))\n",
    "    f_0 = unumpy.uarray(quadratic_model(x*0, *coeffs),t_student*prediction_uncertainty(x*0,covariance))\n",
    "    y_corrected = y_data - (f_x - f_0)\n",
    "\n",
    "    return y_corrected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a3928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Llamar esta función al inicio de tu script\n",
    "configurar_estilo_global()\n",
    "def plot_correction_analysis(x, y_data,y_corrected, coeffs, dataset_name,covariance):\n",
    "    \"\"\"Genera gráfico de la corrección aplicada\"\"\"\n",
    "    # Calcular valores del modelo\n",
    "    x_smooth = np.linspace(0, max(x), 300)\n",
    "    y_fit_smooth = quadratic_model(x_smooth, *coeffs)\n",
    "    f_0 = quadratic_model(0, *coeffs)\n",
    "\n",
    "    fig,ax = plt.subplots(1,1,tight_layout=True,figsize=(12,5))\n",
    "    # Subplot 1: Comparación antes/después de la corrección\n",
    "\n",
    "    dataset_name = dataset_name.replace(\"(_\",\"(\").replace(\"_\",\" \")\n",
    "\n",
    "    \n",
    "    ax.errorbar(x, unumpy.nominal_values(y_corrected), yerr=unumpy.std_devs(y_corrected),\n",
    "                fmt='s', capsize=3, alpha=0.7, color='red', label='Datos corregidos')\n",
    "    ax.axhline(y=f_0, color='r', linestyle='--', label=f'f(0) = {f_0:.3f}', alpha=0.7)\n",
    "    \n",
    "    # Calcular estadísticas de los datos corregidos\n",
    "    y_corr_nominal = unumpy.nominal_values(y_corrected)\n",
    "    y_corr_std = unumpy.nominal_values(y_corrected)\n",
    "    mean_corrected = np.average(y_corr_nominal,weights=y_corr_std**-2)\n",
    "    std_corrected = np.average((y_corr_nominal-mean_corrected)**2,weights=y_corr_std**-2)**0.5\n",
    "    \n",
    "    ax.axhline(y=mean_corrected, color='green', linestyle=':', \n",
    "                label=f'Media ± 1σ = {un.ufloat(mean_corrected,std_corrected):.1uf}', alpha=0.7)\n",
    "    ax.fill_between([min(x), max(x)], \n",
    "                    mean_corrected - std_corrected, \n",
    "                    mean_corrected + std_corrected,\n",
    "                    alpha=0.2, color='green', label='σ')\n",
    "    \n",
    "    ax.set_xlabel('Distancia del centro [px]')\n",
    "    ax.set_ylabel('Relación corregida [px/mm]')\n",
    "    ax.set_title(f'Datos corregidos - {dataset_name}')\n",
    "    ax.legend()\n",
    "    ax.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "    ax.yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "    ax.ticklabel_format(axis='y', style='sci', scilimits=(0,0))\n",
    "    ax.grid(which='major', linestyle='--', alpha=0.7)\n",
    "    ax.grid(which='minor', linestyle=':', alpha=0.5)\n",
    "    \n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # Imprimir estadísticas de la corrección\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(\"ESTADÍSTICAS DE LA CORRECCIÓN\")\n",
    "    print(f\"{'='*40}\")\n",
    "    print(f\"Valor en centro f(0): {f_0:.4f}\")\n",
    "    print(f\"Media de datos corregidos: {mean_corrected:.4f}\")\n",
    "    print(f\"Desviación estándar corregida: {std_corrected:.4f}\")\n",
    "    print(f\"Reducción de variación: {std_corrected/np.std(unumpy.nominal_values(y_data)):.2%}\")\n",
    "    return un.ufloat(mean_corrected,std_corrected)\n",
    "def plot_individual_analysis(x, y_data, sigma_y, y_corrected,coeffs, cov_matrix, residuals, dataset_name):\n",
    "    \"\"\"Genera gráficos individuales para un dataset\"\"\"\n",
    "    fig,ax = plt.subplots(1,2,tight_layout=True,figsize=(17, 5.5))\n",
    "    dataset_name = dataset_name.replace(\"(_\",\"(\").replace(\"_\",\" \")\n",
    "    # Gráfico 1: Residuos\n",
    "    t_student = stats.t.ppf(0.975,len(y_data)-1)\n",
    "    res_nominal = unumpy.nominal_values(residuals)\n",
    "    res_std = unumpy.std_devs(residuals)\n",
    "    ax[0].errorbar(x, res_nominal, yerr=res_std, fmt='o', capsize=3, label='Residuos')\n",
    "    ax[0].axhline(y=0, color='r', linestyle='--', alpha=0.7)\n",
    "    ax[0].set_xlabel('Distancia del centro [px]')\n",
    "    ax[0].set_ylabel('Residuos [px/mm]')\n",
    "    ax[0].set_title(f'Residuos del ajuste - {dataset_name}')\n",
    "    ax[0].legend()\n",
    "    ax[0].xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "    ax[0].yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "    ax[0].ticklabel_format(axis='y', style='sci', scilimits=(0,0))\n",
    "    ax[0].grid(which='major', linestyle='--', alpha=0.7)\n",
    "    ax[0].grid(which='minor', linestyle=':', alpha=0.5)\n",
    "    \n",
    "    # Gráfico 2: Datos y ajuste\n",
    "    \n",
    "    x_smooth = np.linspace(0, max(x), 300)\n",
    "    y_fit_smooth = quadratic_model(x_smooth, *coeffs)\n",
    "    uncertainty = t_student*prediction_uncertainty(x_smooth, cov_matrix)\n",
    "\n",
    "    ax[1].errorbar(x, y_data, yerr=sigma_y, fmt='o', capsize=3, label='Datos experimentales', alpha=0.7)\n",
    "    \n",
    "    ax[1].plot(x_smooth, y_fit_smooth, 'r-', label='Ajuste cuadrático', linewidth=2)\n",
    "    ax[1].fill_between(x_smooth, y_fit_smooth - uncertainty, y_fit_smooth + uncertainty,\n",
    "                    alpha=0.3, label='Intervalo de confianza')\n",
    "    \n",
    "    ax[1].set_xlabel('Distancia del centro [px]')\n",
    "    ax[1].set_ylabel('Relación [px/mm]')\n",
    "    ax[1].set_title(f'Datos y ajuste - {dataset_name}')\n",
    "    ax[1].legend()\n",
    "    ax[1].xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "    ax[1].yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "    ax[1].ticklabel_format(axis='y', style='sci', scilimits=(0,0))\n",
    "    ax[1].grid(which='major', linestyle='--', alpha=0.7)\n",
    "    ax[1].grid(which='minor', linestyle=':', alpha=0.5)\n",
    "def plot_combined_analysis(all_results):\n",
    "    \"\"\"Genera gráfico combinado de todos los datasets\"\"\"\n",
    "\n",
    "    fig,ax =plt.subplots(1,1,tight_layout=True,figsize=(12,8))\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(all_results)))\n",
    "    \n",
    "    for i, (dataset_name, result) in enumerate(all_results.items()):\n",
    "        dataset_name = dataset_name.replace(\"(_\",\"(\").replace(\"_\",\" \")\n",
    "        \n",
    "        x = result['x']\n",
    "        y_data = result['y_data']\n",
    "        coeffs = result['coeffs']\n",
    "        \n",
    "        x_smooth = np.linspace(0, max(x), 300)\n",
    "        y_fit_smooth = quadratic_model(x_smooth, *coeffs)\n",
    "        \n",
    "        # Plot datos\n",
    "        ax.errorbar(x, unumpy.nominal_values(y_data), yerr=unumpy.std_devs(y_data),\n",
    "                    fmt='o', color=colors[i], capsize=3, alpha=0.7,\n",
    "                    label=f'{dataset_name} - Datos')\n",
    "        \n",
    "        # Plot ajuste\n",
    "        ax.plot(x_smooth, y_fit_smooth, color=colors[i], linestyle='-',\n",
    "                label=f'{dataset_name} - Ajuste', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Distancia del centro [px]', fontsize=12)\n",
    "    ax.set_ylabel('Relación [px/mm]', fontsize=12)\n",
    "    ax.set_title('Comparación de todos los ajustes', fontsize=14)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.xaxis.set_minor_locator(AutoMinorLocator(10))\n",
    "    ax.yaxis.set_minor_locator(AutoMinorLocator(10))\n",
    "    ax.ticklabel_format(axis='y', style='sci', scilimits=(0,0))\n",
    "    ax.grid(which='major', linestyle='--', alpha=0.7)\n",
    "    ax.grid(which='minor', linestyle=':', alpha=0.5)\n",
    "    plt.show()\n",
    "def estadisticas(df, nombres, zonas, combined_plot=True, show_correction=True):\n",
    "    \"\"\"\n",
    "    Análisis estadístico para múltiples datasets\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : dict\n",
    "        Diccionario con los datos\n",
    "    nombres : list\n",
    "        Lista de nombres de datasets a analizar\n",
    "    zonas : list\n",
    "        Lista de zonas correspondientes a cada dataset\n",
    "    combined_plot : bool\n",
    "        Si True, genera gráfico combinado al final\n",
    "    show_correction : bool\n",
    "        Si True, genera gráfico detallado de la corrección\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "    correction_results = {}\n",
    "    \n",
    "    for i, (nombre, zona) in enumerate(zip(nombres, zonas)):\n",
    "        print(f\"\\n{'#'*60}\")\n",
    "        print(f\"ANALIZANDO: {nombre} - Zona: {zona}\")\n",
    "        print(f\"{'#'*60}\")\n",
    "        \n",
    "\n",
    "        # Extraer datos\n",
    "        x = df[nombre][zona][\"distancia del centro\"].to_numpy()\n",
    "        y_un = df[nombre][zona][\"px/mm\"].to_numpy()\n",
    "        y_nominal = unumpy.nominal_values(y_un)\n",
    "        y_std = unumpy.std_devs(y_un)\n",
    "        \n",
    "        # Ajuste cuadrático\n",
    "        coeffs, cov_matrix,ddof = fit_quadratic(x, y_nominal, y_std)\n",
    "        t_student = stats.t.ppf(0.975,len(y_nominal)-1)\n",
    "        # Predicciones e incertidumbres\n",
    "        y_pred = unumpy.uarray(quadratic_model(x, *coeffs), \n",
    "                                t_student*prediction_uncertainty(x, cov_matrix))\n",
    "        \n",
    "        # Estadísticas\n",
    "        residuals, r_squared,chi2_red = calculate_statistics(y_un, y_pred,ddof)\n",
    "\n",
    "        ##Aplicamos la corrección\n",
    "        y_corrected= apply_correction(y_un, x, coeffs,cov_matrix)\n",
    "        ## Buscamos outliers\n",
    "        y_corrected_n = unumpy.nominal_values(y_corrected)\n",
    "        y_corrected_std = unumpy.std_devs(y_corrected)\n",
    "        \n",
    "        y_un,x,borra,outliers = MAD_Detection(x,y_corrected_n,y_std,y_nominal)\n",
    "        \n",
    "        y_std = unumpy.std_devs(y_un)\n",
    "\n",
    "        y_nominal = unumpy.nominal_values(y_un)\n",
    "        \n",
    "\n",
    "        if borra:\n",
    "            #Ajustamos el modelo\n",
    "            coeffs, cov_matrix,ddof = fit_quadratic(x, y_nominal, y_std)\n",
    "            t_student = stats.t.ppf(0.975,len(y_nominal)-1)\n",
    "            #Hacemos la predicciom\n",
    "            y_pred = unumpy.uarray(quadratic_model(x, *coeffs),t_student*prediction_uncertainty(x, cov_matrix))\n",
    "            \n",
    "            #Calculamos estadisticos\n",
    "            residuals, r_squared,chi2_red = calculate_statistics(y_un, y_pred,ddof)\n",
    "            \n",
    "            #Aplicamos corrección\n",
    "            y_corrected = apply_correction(y_un, x, coeffs,cov_matrix)\n",
    "            y_corrected_n = unumpy.nominal_values(y_corrected)\n",
    "            y_corrected_std = unumpy.std_devs(y_corrected)\n",
    "            #Buscamos el valor global de la imagen\n",
    "\n",
    "\n",
    "        stats.probplot(y_corrected_n,dist=\"norm\",plot=pylab)\n",
    "        pylab.show()\n",
    "        \n",
    "        # Resultados\n",
    "        p_test = stats.chi2.sf(chi2_red*ddof,ddof)\n",
    "        print(f\"R² del ajuste: {r_squared:.4f}\\nY el chi² reducido es: {chi2_red}\\np-valor de chi²: {p_test}\")\n",
    "        coeffs_un = print_fit_results(coeffs, cov_matrix, ddof)\n",
    "        # Gráfico individual\n",
    "\n",
    "        plot_individual_analysis(x, y_nominal, y_std,y_corrected, coeffs, cov_matrix, \n",
    "                                residuals, f\"{nombre} ({zona})\")\n",
    "\n",
    "        # Gráfico de corrección detallado (separado)\n",
    "        if show_correction:\n",
    "            media_total = plot_correction_analysis(x, y_un, y_corrected, coeffs, \n",
    "                                                                    f\"{nombre} ({zona})\",cov_matrix)\n",
    "\n",
    "            correction_results[f\"{nombre} ({zona})\"] = {\n",
    "                'y_corrected': y_corrected,\n",
    "                'std_original': np.std(y_nominal),\n",
    "                'f_0': quadratic_model(x, *coeffs),\n",
    "                'outliers':outliers,\n",
    "                'Valor_de_la_zona':media_total\n",
    "            }\n",
    "\n",
    "        # Guardar resultados para gráfico combinado\n",
    "        dataset_name = f\"{nombre} ({zona})\"\n",
    "        all_results[dataset_name] = {\n",
    "            'x': x,\n",
    "            'y_data': y_un,\n",
    "            'coeffs': coeffs,\n",
    "            'cov_matrix': cov_matrix,\n",
    "            'residuals': residuals\n",
    "        }\n",
    "        #except Exception as e:\n",
    "        #    print(f\"❌ Error procesando {nombre}[{zona}]: {e}\")\n",
    "        #    continue\n",
    "    \n",
    "    # Gráfico combinado si hay múltiples datasets\n",
    "    if combined_plot and len(all_results) > 1:\n",
    "        print(f\"\\n{'#'*60}\")\n",
    "        print(\"GENERANDO GRÁFICO COMBINADO\")\n",
    "        print(f\"{'#'*60}\")\n",
    "        plot_combined_analysis(all_results)\n",
    "    \n",
    "    return all_results, correction_results\n",
    "\n",
    "# Análisis múltiple\n",
    "print(\"\\n=== ANÁLISIS MÚLTIPLE 50X ===\")\n",
    "resultados_50_Pelos, correcciones_50_Pelos = estadisticas(resultados_50, \n",
    "                                        [\"50 P_1\", \"50 A2_1\"], \n",
    "                                        [\"_Pelos_X\", \"_Pelos_X\"], \n",
    "                                        combined_plot=True, show_correction=False)\n",
    "# Análisis múltiple\n",
    "print(\"\\n=== ANÁLISIS MÚLTIPLE 20X ===\")\n",
    "resultados_20_Pelos, correcciones_20_Pelos = estadisticas(resultados_20, \n",
    "                                        [\"20 A2_1\", \"20 P_1\"], \n",
    "                                        [\"_Pelos_X\", \"_Pelos_X\"], \n",
    "                                        combined_plot=True, show_correction=False)\n",
    "# Análisis múltiple\n",
    "print(\"\\n=== ANÁLISIS MÚLTIPLE 50X zona 1 y 2 ===\")\n",
    "resultados_50_Pelos_Comp, correcciones_50_Pelos_Comp = estadisticas(resultados_50, \n",
    "                                        [\"50 A2_1\", \"50 A2_2\"], \n",
    "                                        [\"_Pelos_X\", \"_Pelos_X2\"], \n",
    "                                        combined_plot=True, show_correction=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9637d53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20_corr = pd.DataFrame(correcciones_20_Pelos)\n",
    "df_50_corr = pd.DataFrame(correcciones_50_Pelos)\n",
    "df_50_corr_comp = pd.DataFrame(correcciones_50_Pelos_Comp)\n",
    "display(df_20_corr)\n",
    "display(df_50_corr)\n",
    "display(df_50_corr_comp)\n",
    "def weight_ufloat(Val,Sigma,n=-2,Print=False):\n",
    "    if isinstance(Val,list):\n",
    "        Val = np.array(Val)\n",
    "        Sigma = np.array(Sigma)\n",
    "    W = Sigma**n\n",
    "    mean = np.average(Val,weights=W)\n",
    "    Var = np.average((Val-mean)**2,weights=W)\n",
    "    if Print:\n",
    "        return pretty_unc(un.ufloat(mean,Var**0.5))\n",
    "    else:\n",
    "        return un.ufloat(mean,Var**0.5)\n",
    "\n",
    "\n",
    "def Histogramas_finales(df):\n",
    "    claves = df.columns.values.tolist()\n",
    "    valores_n = []\n",
    "    valores_s = []\n",
    "    labels = []\n",
    "    \n",
    "    for key in claves:\n",
    "        labels.append(key.replace(\"(_\",\"(\").replace(\"_\",\" \"))\n",
    "    x = np.arange(len(labels))\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    for key in claves:\n",
    "\n",
    "        valores_n.append(df[key][\"Valor_de_la_zona\"].n)\n",
    "        valores_s.append(df[key][\"Valor_de_la_zona\"].s)\n",
    "    Prom = weight_ufloat(valores_n,valores_s,Print=False)\n",
    "    print(pretty_unc(Prom))\n",
    "    ax.bar(x, valores_n, yerr=valores_s, capsize=4, color=\"#4C72B0\")\n",
    "    plt.axhline(y=Prom.n, color='red', linestyle=':', \n",
    "                label=f'Media ± 1σ = {pretty_unc(Prom)}', alpha=0.7)\n",
    "    plt.fill_between([min(x)-0.5, max(x)+0.5], \n",
    "                    Prom.n - Prom.s, \n",
    "                    Prom.n + Prom.s,\n",
    "                    alpha=0.5, color='green', label=f'±1σ ')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(\"px/mm\")\n",
    "    ax.set_title(\"Estadística por zona/efecto\")\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "Histogramas_finales(df_20_corr)\n",
    "Histogramas_finales(df_50_corr)\n",
    "Histogramas_finales(df_50_corr_comp)\n",
    "print(pretty_unc(df_20_corr[\"20 P_1 (_Pelos_X)\"][\"Valor_de_la_zona\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272b0819",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Estos son los cuadrados en longitudes\n",
    "#Cuadrado_50 = [resultados_50[\"50 A2_1\"][\"_Cuadrado_X\"][\"px/mm\"].to_numpy()*0.03093]\n",
    "#Cuadrado_50.append(resultados_50[\"50 A2_1\"][\"_Cuadrado_Y\"][\"px/mm\"].to_numpy()*0.03054)\n",
    "#Cuadrado_20 = [resultados_20[\"20 A2_1\"][\"_Cuadrado_X\"][\"px/mm\"].to_numpy()*0.03093]\n",
    "#Cuadrado_20.append(resultados_20[\"20 A2_1\"][\"_Cuadrado_Y\"][\"px/mm\"].to_numpy()*0.03054)\n",
    "# En relación px/mm\n",
    "Cuadrado_50 = [resultados_50[\"50 A2_1\"][\"_Cuadrado_X\"][\"px/mm\"].to_numpy()]\n",
    "Cuadrado_50.append(resultados_50[\"50 A2_1\"][\"_Cuadrado_Y\"][\"px/mm\"].to_numpy())\n",
    "\n",
    "Cuadrado_20 = [resultados_20[\"20 A2_1\"][\"_Cuadrado_X\"][\"px/mm\"].to_numpy()]\n",
    "Cuadrado_20.append(resultados_20[\"20 A2_1\"][\"_Cuadrado_Y\"][\"px/mm\"].to_numpy())\n",
    "\n",
    "C_50_n ,C_50_s= [val[0].n for val in Cuadrado_50],[val[0].s for val in Cuadrado_50]\n",
    "\n",
    "C_20_n, C_20_s = [val[0].n for val in Cuadrado_20],[val[0].s for val in Cuadrado_20]\n",
    "print(\"###################################### Cuadrados #######################################\")\n",
    "print(\"######################################### 50X ##########################################\")\n",
    "print(C_50_n,C_50_s)\n",
    "print(weight_ufloat(C_50_n,C_50_s))\n",
    "print(\"###################################### Cuadrados #######################################\")\n",
    "print(\"######################################### 20X ##########################################\")\n",
    "print(C_20_n)\n",
    "print(weight_ufloat(C_20_n,C_20_s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87340b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cuadrado X, 50X: \",pretty_unc(resultados_50[\"50 A2_1\"][\"_Cuadrado_X\"][\"px/mm\"].to_numpy()))\n",
    "print(\"Cuadrado X, 20X: \", pretty_unc(317/resultados_20[\"20 A2_1\"][\"_Cuadrado_X\"][\"px/mm\"].to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba87194",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cuadrado Y, 50X: \",pretty_unc(resultados_50[\"50 A2_1\"][\"_Cuadrado_Y\"][\"px/mm\"].to_numpy()))\n",
    "print(\"Cuadrado Y, 20X: \", pretty_unc(resultados_20[\"20 A2_1\"][\"_Cuadrado_Y\"][\"px/mm\"].to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe4ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "317/17550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84526b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_50[\"50 A2_1\"][\"_Cuadrado_X\"][\"px/mm\"].to_numpy()*0.03093"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb74d55f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d59bef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "Mediciones_20_or = pd.DataFrame({\"B_NE_X\":0.0447,\"B_SE_X\":0.0447,\"P_CE_Y\":0.0293,\"C_C_Y\":0.002,\"G_CW_X\":0.0225,\"A_SW_Y\":0.0238,\"A_SE_Y\":0.0238},index=[\"distancias 20X\"])\n",
    "Mediciones_20_or=Mediciones_20_or.transpose()*1000.0\n",
    "\n",
    "display(Mediciones_20_or)\n",
    "Mediciones_50_or = pd.DataFrame({\"B_NE_X\":0.0447,\"B_SE_X\":0.0447,\"P_CE_Y\":0.0293,\"C_C_Y\":0.002,\"G_CW_X\":0.0225,\"A_SW_Y\":0.0238,\"A_SE_Y\":0.0238},index=[\"distancias 50X\"])\n",
    "Mediciones_50_or=Mediciones_50_or.transpose()*1000.0\n",
    "display(Mediciones_50_or)\n",
    "print(\"Antes del revelado\")\n",
    "df_original=pd.merge(Mediciones_20_or,Mediciones_50_or,on=Mediciones_50_or.index)\n",
    "\n",
    "\n",
    "\n",
    "Mediciones_20 = {\"B_NE_X\":354,\"B_SE_X\":352,\"P_CE_Y\":289,\"C_C_Y\":73,\"G_CW_X\":128,\"A_SW_Y\":84,\"A_SE_Y\":139}\n",
    "Mediciones_20 = pd.DataFrame(Mediciones_20,index=[0])\n",
    "Mediciones_20 = Mediciones_20.transpose()\n",
    "valor = [Mediciones_20[0][i] / 6.940 if \"X\" in i else Mediciones_20[0][i] / 7.090 for i in Mediciones_20.index.values.astype(str)]\n",
    "\n",
    "Mediciones_20[\"distancias 20X\"]= pd.Series(valor, index=Mediciones_20.index)\n",
    "\n",
    "display(Mediciones_20)\n",
    "Mediciones_50 = {\"B_NE_X\":887,\"B_SE_X\":np.nan,\"P_CE_Y\":745,\"C_C_Y\":186,\"G_CW_X\":317,\"A_SW_Y\":213,\"A_SE_Y\":359}\n",
    "Mediciones_50 = pd.DataFrame(Mediciones_50,index=[0])\n",
    "Mediciones_50 = Mediciones_50.transpose()\n",
    "valor = [Mediciones_50[0][i] / 17.440 if \"X\" in i else Mediciones_50[0][i] / 17.710 for i in Mediciones_50.index.values.astype(str)]\n",
    "\n",
    "Mediciones_50[\"distancias 50X\"]= pd.Series(valor, index=Mediciones_50.index)\n",
    "\n",
    "display(Mediciones_50)\n",
    "print(\"Antes del revelado\")\n",
    "df_before=pd.merge(Mediciones_20,Mediciones_50,on=Mediciones_50.index)\n",
    "\n",
    "\n",
    "Mediciones_20_PR = {\"B_NE_X\":405,\"B_SE_X\":385,\"P_CE_Y\":303,\"C_C_Y\":np.nan,\"G_CW_X\":np.nan,\"A_SW_Y\":np.nan,\"A_SE_Y\":142}\n",
    "Mediciones_50_PR = {\"B_NE_X\":np.nan,\"B_SE_X\":np.nan,\"P_CE_Y\":864,\"C_C_Y\":np.nan,\"G_CW_X\":np.nan,\"A_SW_Y\":332,\"A_SE_Y\":370}\n",
    "\n",
    "\n",
    "Mediciones_20_PR = pd.DataFrame(Mediciones_20_PR,index=[0])\n",
    "Mediciones_20_PR = Mediciones_20_PR.transpose()\n",
    "valor = [Mediciones_20_PR[0][i] / 6.940 if \"X\" in i else Mediciones_20_PR[0][i] / 7.090 for i in Mediciones_20_PR.index.values.astype(str)]\n",
    "Mediciones_20_PR[\"distancias 20X\"]= pd.Series(valor, index=Mediciones_20_PR.index).values\n",
    "display(Mediciones_20_PR)\n",
    "Mediciones_50_PR = pd.DataFrame(Mediciones_50_PR,index=[0])\n",
    "Mediciones_50_PR = Mediciones_50_PR.transpose()\n",
    "valor = [Mediciones_50_PR[0][i] / 17.440 if \"X\" in i else Mediciones_50_PR[0][i] / 17.710 for i in Mediciones_50_PR.index.values.astype(str)]\n",
    "Mediciones_50_PR[\"distancias 50X\"]= pd.Series(valor, index=Mediciones_50_PR.index).values\n",
    "display(Mediciones_50_PR)\n",
    "print(\"Despues del revelado\")\n",
    "df_after=pd.merge(Mediciones_20_PR,Mediciones_50_PR,on=Mediciones_50_PR.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5bbae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"original:\")\n",
    "display(df_original)\n",
    "print(\"before:\")\n",
    "display(df_before)\n",
    "print(\"after:\")\n",
    "display(df_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45af0ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.formula.api import mixedlm\n",
    "\n",
    "# Asumimos que ya existen:\n",
    "# df_original, df_before, df_after\n",
    "# y que todas tienen columnas como:\n",
    "#   key_0, distancias 20X, distancias 50X\n",
    "\n",
    "# --- Paso 1: Verificamos qué columnas hay realmente ---\n",
    "print(\"Columnas en df_original:\", df_original.columns.tolist())\n",
    "print(\"Columnas en df_before:\", df_before.columns.tolist())\n",
    "print(\"Columnas en df_after:\", df_after.columns.tolist())\n",
    "\n",
    "# --- Paso 2: Unificamos en formato largo ---\n",
    "dfs = []\n",
    "for name, df in [(\"before\", df_before), (\"after\", df_after)]:\n",
    "    temp = df.melt(id_vars=\"key_0\", var_name=\"mag\", value_name=\"value\")\n",
    "    temp[\"time\"] = name\n",
    "    dfs.append(temp)\n",
    "\n",
    "long = pd.concat(dfs, ignore_index=True)\n",
    "print(\"\\nVista previa del DataFrame largo:\")\n",
    "display(long.head(10))\n",
    "\n",
    "# Eliminamos filas vacías\n",
    "long = long.dropna(subset=[\"value\"])\n",
    "print(f\"Tamaño después de limpiar: {len(long)} observaciones válidas\")\n",
    "\n",
    "# Codificamos 'time' como variable numérica\n",
    "long[\"time_code\"] = long[\"time\"].map({\"before\": 0, \"after\": 1})\n",
    "\n",
    "# --- Paso 3: Ajustamos el modelo mixto por magnificación ---\n",
    "for mag in long[\"mag\"].unique():\n",
    "    sub = long[long[\"mag\"] == mag]\n",
    "    print(f\"\\n=== MixedLM para {mag} ===\")\n",
    "    print(f\"Datos: n={len(sub)} sujetos={sub['key_0'].nunique()}\")\n",
    "    \n",
    "    if len(sub) < 6:\n",
    "        print(\"Datos insuficientes para ajustar un modelo confiable.\\n\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        display(sub)\n",
    "        md = mixedlm(\"value ~ time_code\", sub, groups=sub[\"key_0\"])\n",
    "        mdf = md.fit(reml=False)\n",
    "        print(mdf.summary())\n",
    "    except Exception as e:\n",
    "        print(\"Error en el ajuste:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c83862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.formula.api import mixedlm\n",
    "\n",
    "# Asumimos que ya existen:\n",
    "# df_original, df_before, df_after\n",
    "# y que todas tienen columnas como:\n",
    "#   key_0, distancias 20X, distancias 50X\n",
    "\n",
    "# --- Paso 1: Verificamos qué columnas hay realmente ---\n",
    "print(\"Columnas en df_original:\", df_original.columns.tolist())\n",
    "print(\"Columnas en df_before:\", df_before.columns.tolist())\n",
    "print(\"Columnas en df_after:\", df_after.columns.tolist())\n",
    "\n",
    "# --- Paso 2: Unificamos en formato largo ---\n",
    "dfs = []\n",
    "for name, df in [(\"original\", df_original), (\"after\", df_after)]:\n",
    "    temp = df.melt(id_vars=\"key_0\", var_name=\"mag\", value_name=\"value\")\n",
    "    temp[\"time\"] = name\n",
    "    dfs.append(temp)\n",
    "\n",
    "long = pd.concat(dfs, ignore_index=True)\n",
    "print(\"\\nVista previa del DataFrame largo:\")\n",
    "display(long.head(10))\n",
    "\n",
    "# Eliminamos filas vacías\n",
    "long = long.dropna(subset=[\"value\"])\n",
    "print(f\"Tamaño después de limpiar: {len(long)} observaciones válidas\")\n",
    "\n",
    "# Codificamos 'time' como variable numérica\n",
    "long[\"time_code\"] = long[\"time\"].map({\"original\": 0, \"after\": 1})\n",
    "\n",
    "# --- Paso 3: Ajustamos el modelo mixto por magnificación ---\n",
    "for mag in long[\"mag\"].unique():\n",
    "    sub = long[long[\"mag\"] == mag]\n",
    "    print(f\"\\n=== MixedLM para {mag} ===\")\n",
    "    print(f\"Datos: n={len(sub)} sujetos={sub['key_0'].nunique()}\")\n",
    "    \n",
    "    if len(sub) < 6:\n",
    "        print(\"Datos insuficientes para ajustar un modelo confiable.\\n\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        display(sub)\n",
    "        md = mixedlm(\"value ~ time_code\", sub, groups=sub[\"key_0\"])\n",
    "        mdf = md.fit(reml=False)\n",
    "        print(mdf.summary())\n",
    "    except Exception as e:\n",
    "        print(\"Error en el ajuste:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4529478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.formula.api import mixedlm\n",
    "\n",
    "# Asumimos que ya existen:\n",
    "# df_original, df_before, df_after\n",
    "# y que todas tienen columnas como:\n",
    "#   key_0, distancias 20X, distancias 50X\n",
    "\n",
    "# --- Paso 1: Verificamos qué columnas hay realmente ---\n",
    "print(\"Columnas en df_original:\", df_original.columns.tolist())\n",
    "print(\"Columnas en df_before:\", df_before.columns.tolist())\n",
    "print(\"Columnas en df_after:\", df_after.columns.tolist())\n",
    "\n",
    "# --- Paso 2: Unificamos en formato largo ---\n",
    "dfs = []\n",
    "for name, df in [(\"original\", df_original), (\"before\", df_before)]:\n",
    "    temp = df.melt(id_vars=\"key_0\", var_name=\"mag\", value_name=\"value\")\n",
    "    temp[\"time\"] = name\n",
    "    dfs.append(temp)\n",
    "\n",
    "long = pd.concat(dfs, ignore_index=True)\n",
    "print(\"\\nVista previa del DataFrame largo:\")\n",
    "display(long.head(10))\n",
    "\n",
    "# Eliminamos filas vacías\n",
    "long = long.dropna(subset=[\"value\"])\n",
    "print(f\"Tamaño después de limpiar: {len(long)} observaciones válidas\")\n",
    "\n",
    "# Codificamos 'time' como variable numérica\n",
    "long[\"time_code\"] = long[\"time\"].map({\"before\": 1, \"original\": 0})\n",
    "\n",
    "# --- Paso 3: Ajustamos el modelo mixto por magnificación ---\n",
    "for mag in long[\"mag\"].unique():\n",
    "    sub = long[long[\"mag\"] == mag]\n",
    "    print(f\"\\n=== MixedLM para {mag} ===\")\n",
    "    print(f\"Datos: n={len(sub)} sujetos={sub['key_0'].nunique()}\")\n",
    "    \n",
    "    if len(sub) < 6:\n",
    "        print(\"Datos insuficientes para ajustar un modelo confiable.\\n\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        display(sub)\n",
    "        md = mixedlm(\"value ~ time_code\", sub, groups=sub[\"key_0\"])\n",
    "        mdf = md.fit(reml=False)\n",
    "        print(mdf.summary())\n",
    "    except Exception as e:\n",
    "        print(\"Error en el ajuste:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8328e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.formula.api import mixedlm\n",
    "\n",
    "# ==========================================================\n",
    "# 1) MERGE ROBUSTO: detecta columnas automáticamente\n",
    "# ==========================================================\n",
    "# asumimos que cada df tiene: \"key_0\", \"distancias 20\", \"distancias 50\"\n",
    "# o nombres similares; los renombramos con sufijos\n",
    "df_o = df_original.add_suffix(\"_original\").rename(columns={\"key_0_original\": \"key_0\"})\n",
    "df_b = df_before.add_suffix(\"_before\").rename(columns={\"key_0_before\": \"key_0\"})\n",
    "df_a = df_after.add_suffix(\"_after\").rename(columns={\"key_0_after\": \"key_0\"})\n",
    "\n",
    "# combinamos todo\n",
    "df = df_o.merge(df_b, on=\"key_0\", how=\"outer\").merge(df_a, on=\"key_0\", how=\"outer\")\n",
    "\n",
    "# nombres base (sin sufijo)\n",
    "cols = []\n",
    "for c in df.columns:\n",
    "    if \"distancias\" in c and (\"20\" in c or \"50\" in c):\n",
    "        base = c.split(\"_\")[0] + \" \" + c.split(\"_\")[1]  # \"distancias 20\" o \"distancias 50\"\n",
    "        if base not in cols:\n",
    "            cols.append(base)\n",
    "\n",
    "print(\"Columnas base detectadas:\", cols)\n",
    "print(\"Tamaño final del merge:\", df.shape)\n",
    "\n",
    "# ==========================================================\n",
    "# 2) CONVERSIÓN A FORMATO LONG\n",
    "# ==========================================================\n",
    "rows = []\n",
    "for _, row in df.iterrows():\n",
    "    for c in cols:\n",
    "        val_o = row.get(f\"{c}_original\", np.nan)\n",
    "        val_b = row.get(f\"{c}_before\", np.nan)\n",
    "        val_a = row.get(f\"{c}_after\", np.nan)\n",
    "        rows.append({\"key_0\": row[\"key_0\"], \"mag\": c, \"time\": \"original\", \"value\": val_o})\n",
    "        rows.append({\"key_0\": row[\"key_0\"], \"mag\": c, \"time\": \"before\", \"value\": val_b})\n",
    "        rows.append({\"key_0\": row[\"key_0\"], \"mag\": c, \"time\": \"after\", \"value\": val_a})\n",
    "\n",
    "long = pd.DataFrame(rows)\n",
    "print(\"\\nVista previa de 'long' antes de limpiar:\")\n",
    "print(long.head(10))\n",
    "\n",
    "# quitamos filas sin valor medido\n",
    "long = long.dropna(subset=[\"value\"])\n",
    "print(f\"\\nTamaño final del long: {len(long)} filas (sin NaN en 'value')\")\n",
    "\n",
    "# ==========================================================\n",
    "# 3) MODELO MIXTO\n",
    "# ==========================================================\n",
    "print(\"\\n=== Modelo lineal mixto (MixedLM) ===\")\n",
    "\n",
    "for c in cols:\n",
    "    sub = long[long[\"mag\"] == c]\n",
    "    if len(sub) < 3:\n",
    "        print(f\"{c}: muy pocos datos (n={len(sub)})\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Modelo con factor categórico 'time' y efecto aleatorio por zona\n",
    "        model = mixedlm(\"value ~ C(time)\", sub, groups=sub[\"key_0\"])\n",
    "        result = model.fit(reml=False)\n",
    "        print(f\"\\n--- {c} ---\")\n",
    "        print(result.summary())\n",
    "    except Exception as e:\n",
    "        print(f\"{c}: error en ajuste -> {e}\")\n",
    "\n",
    "# ==========================================================\n",
    "# 4) INTERPRETACIÓN\n",
    "# ==========================================================\n",
    "print(\"\"\"\n",
    "Interpretación:\n",
    "- Intercept: promedio del grupo base ('original').\n",
    "- C(time)[T.before]: cambio medio de 'before' respecto a 'original'.\n",
    "- C(time)[T.after]: cambio medio de 'after' respecto a 'original'.\n",
    "- Group Var: varianza entre zonas (efecto aleatorio).\n",
    "- Residual: variación intra-zona (ruido experimental).\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5febab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.formula.api import mixedlm\n",
    "\n",
    "df = pd.merge(df_original, df_after, on=\"key_0\", suffixes=(\"_original\", \"_after\"))\n",
    "df = pd.merge(df, df_before, on=\"key_0\")\n",
    "print(\"Merge realizado entre df_before y df_after -> df\")\n",
    "cols = [\"distancias 20\", \"distancias 50\"]\n",
    "display(df)\n",
    "rows = []\n",
    "for _, row in df.iterrows():\n",
    "    for c in cols:\n",
    "        rows.append({\n",
    "            \"key_0\": row[\"key_0\"],\n",
    "            \"mag\": c,\n",
    "            \"time\": \"original\",\n",
    "            \"value\": row.get(f\"{c}_original\", np.nan)\n",
    "        })\n",
    "        rows.append({\n",
    "            \"key_0\": row[\"key_0\"],\n",
    "            \"mag\": c,\n",
    "            \"time\": \"before\",\n",
    "            \"value\": row.get(f\"{c}_before\", np.nan)\n",
    "        })\n",
    "        rows.append({\n",
    "            \"key_0\": row[\"key_0\"],\n",
    "            \"mag\": c,\n",
    "            \"time\": \"after\",\n",
    "            \"value\": row.get(f\"{c}_after\", np.nan)\n",
    "        })\n",
    "long = pd.DataFrame(rows)\n",
    "long_obs = long.dropna(subset=[\"value\"]).copy()\n",
    "# codifica time\n",
    "long_obs[\"time_code\"] = long_obs[\"time\"].map({\"before\":np.nan,\"after\":1})\n",
    "display(long)\n",
    "print(\"\\n=== Mixed models (por magnificación) ===\")\n",
    "for c in cols:\n",
    "    sub = long_obs[long_obs[\"mag\"] == c]\n",
    "    if len(sub) < 2:\n",
    "        print(f\"{c}: datos insuficientes para MixedLM (n={len(sub)})\")\n",
    "        continue\n",
    "    try:\n",
    "        md = mixedlm(\"value ~ time_code\", sub, groups=sub[\"key_0\"])\n",
    "        mdf = md.fit(reml=False)\n",
    "        print(f\"\\nMixedLM para {c}:\")\n",
    "        print(mdf.summary())\n",
    "    except Exception as e:\n",
    "        print(f\"Error ajustando MixedLM para {c}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60725603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_rel, wilcoxon\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import mixedlm\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# -------------------------\n",
    "# 0) Carga / Preparación\n",
    "# -------------------------\n",
    "# Ajusta aquí cómo cargas tus datos. Dos opciones:\n",
    "# 1) Si tienes dos dataframes separados: df_before, df_after (ambos con columna key_0)\n",
    "#    df_before = pd.read_csv(\"antes.csv\"); df_after = pd.read_csv(\"despues.csv\")\n",
    "# 2) Si ya tienes un único df con columnas *_before y *_after, úsalo tal cual.\n",
    "#\n",
    "# Este bloque intenta usar lo que tengas en memoria:\n",
    "\n",
    "\n",
    "try:\n",
    "    df = pd.merge(df_original, df_before, on=\"key_0\", suffixes=(\"_original\", \"_before\"))\n",
    "\n",
    "    print(\"Merge realizado entre df_before y df_after -> df\")\n",
    "    display(df)\n",
    "except NameError:\n",
    "    raise RuntimeError(\"No se encontró 'df' ni 'df_before/df_after'. Carga tus datos antes de ejecutar.\")\n",
    "\n",
    "# columnas que usaremos (ajusta si tus nombres son distintos)\n",
    "cols = [\"distancias 20\", \"distancias 50\"]\n",
    "\n",
    "# Si en tu tabla los ceros en 'after' significan 'no medido', conviértelos a NaN.\n",
    "# Si hay ceros válidos, quita / comenta este bloque.\n",
    "for col in cols:\n",
    "    col_after = f\"{col}_after\"\n",
    "    if col_after in df.columns:\n",
    "        # evitar chained assignment => asignar el resultado\n",
    "        df[col_after] = df[col_after].replace(0, np.nan)\n",
    "\n",
    "# quick sanity\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 1) Exploración de missing\n",
    "# -------------------------\n",
    "print(\"\\nResumen de missing en columnas 'after':\")\n",
    "for c in cols:\n",
    "    cname = f\"{c}_after\"\n",
    "    if cname in df.columns:\n",
    "        print(f\" {c}: {df[cname].isna().sum()} / {len(df)} (missing)\")\n",
    "\n",
    "\n",
    "\n",
    "# 5) Modelo mixto (usa todos los datos observados)\n",
    "# -------------------------\n",
    "# Construcción manual del formato long (cada magnificación por fila)\n",
    "rows = []\n",
    "for _, row in df.iterrows():\n",
    "    for c in cols:\n",
    "        rows.append({\n",
    "            \"key_0\": row[\"key_0\"],\n",
    "            \"mag\": c,\n",
    "            \"time\": \"before\",\n",
    "            \"value\": row.get(f\"{c}_original\", np.nan)\n",
    "        })\n",
    "        rows.append({\n",
    "            \"key_0\": row[\"key_0\"],\n",
    "            \"mag\": c,\n",
    "            \"time\": \"after\",\n",
    "            \"value\": row.get(f\"{c}_after\", np.nan)\n",
    "        })\n",
    "long = pd.DataFrame(rows)\n",
    "long_obs = long.dropna(subset=[\"value\"]).copy()\n",
    "# codifica time\n",
    "long_obs[\"time_code\"] = long_obs[\"time\"].map({\"before\":np.nan,\"after\":1})\n",
    "\n",
    "print(\"\\n=== Mixed models (por magnificación) ===\")\n",
    "for c in cols:\n",
    "    sub = long_obs[long_obs[\"mag\"] == c]\n",
    "    if len(sub) < 2:\n",
    "        print(f\"{c}: datos insuficientes para MixedLM (n={len(sub)})\")\n",
    "        continue\n",
    "    try:\n",
    "        md = mixedlm(\"value ~ time_code\", sub, groups=sub[\"key_0\"])\n",
    "        mdf = md.fit(reml=False)\n",
    "        print(f\"\\nMixedLM para {c}:\")\n",
    "        print(mdf.summary())\n",
    "    except Exception as e:\n",
    "        print(f\"Error ajustando MixedLM para {c}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b024d94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import AutoMinorLocator\n",
    "def plot_paired(df, col, annotate_key=True,be=\"before\",af=\"after\"):\n",
    "    fig,ax=plt.subplots(figsize=(6,6),dpi=120)\n",
    "    color=[\"blue\",\"green\",\"red\"]\n",
    "    i=0\n",
    "    for _, row in df.iterrows():\n",
    "        original = row.get(f\"{col}_original\", np.nan)*1000.0\n",
    "        before = row.get(f\"{col}_before\", np.nan)*1000.0\n",
    "        after  = row.get(f\"{col}_after\", np.nan)*1000.0\n",
    "        k = row.get(\"key_0\", \"\").replace(\"_X\",\"\").replace(\"_Y\",\"\").replace(\"_\",\" \")\n",
    "        original_na = pd.isna(original)\n",
    "        before_na = pd.isna(before)\n",
    "        after_na  = pd.isna(after)\n",
    "        if (not before_na) and ( after_na):\n",
    "            ax.plot([0,1], [original,before], marker='o', alpha=0.8,label=f\"{k}\")\n",
    "        if (not before_na) and (not after_na):\n",
    "            ax.plot([0,1,2], [original,before, after], marker='o', alpha=0.8,label=f\"{k}\")\n",
    "        if ( before_na) and ( after_na):\n",
    "            ax.plot(0, original, marker='o',alpha=0.8,label=f\"{k} (sin medida)\")\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    ax.set_xticks([0,1,2], [\"original\",\"Antes del\\nRevelado\",\"Después del\\nRevelado\"])\n",
    "    plt.title(f\"{col.replace(\"_\",\" \").replace(\"milimetros\",\"Mediciones\")}\")\n",
    "    plt.ylabel(\"Medida [μm]\")\n",
    "    plt.xlim(-0.3, 2.6)\n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.legend(fontsize=7)\n",
    "    ax.yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "    plt.grid(which='major', linestyle='--', alpha=0.7)\n",
    "    plt.grid(which='minor', linestyle=':', alpha=0.5)\n",
    "    plt.show()\n",
    "cols = [\"milimetros_20X\", \"milimetros_50X\"]\n",
    "df = pd.merge(df_original, df_before, on=\"key_0\", suffixes=(\"_original\", \"_before\"))\n",
    "df = pd.merge(df, df_after, on=\"key_0\")\n",
    "print(\"Merge realizado entre df_before y df_after -> df\")\n",
    "display(df)\n",
    "for c in cols:\n",
    "    plot_paired(df, c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c54a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
